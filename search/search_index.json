{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SLURMY - Special handLer for Universal Running of Multiple jobs, Yes! Slurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only Slurm and HTCondor are supported as backends, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs. Installation First off, it is very much recommended to use slurmy in python3. While it is compatible with python2, some features only work (properly) in python3. You can either install slurmy using pip or directly check out the git repository. pip Most likely you won't have root privileges on your machine, so you have to make a user pip installation: pip install --user slurmy Depending on your setup pip points to the python2 or python3 pip executable. In order to ensure that you're installing slurmy for python3 you can be more explicit: pip3 install --user slurmy This will install the slurmy module in ~/.local/lib/pythonX.Y/site-packages/ ( X.Y indicates the version of your python installation) and the slurmy executables in ~/.local/bin/ . In order to ensure that python properly finds the module and you can run the executables, you should add the respective paths to PYTHONPATH and PATH : MYPYTHONVERSION=python$(python3 -c \"from sys import version_info; print('{}.{}'.format(version_info.major, version_info.minor))\") export PYTHONPATH=~/.local/lib/$MYPYTHONVERSION/site-packages:$PYTHONPATH export PATH=~/.local/bin:$PATH github Clone the latest stable tag or (if you're brave) master branch locally: git clone https://github.com/Thomas-Maier/slurmy.git LATESTTAG=$(git describe --abbrev=0 --tags) git co $LATESTTAG Make sure to add the respository folder to PYTHONPATH and the bin/ folder to PATH ': ## Assuming that we are in the repository folder now export PYTHONPATH=$PWD:$PYTHONPATH export PATH=$PWD/bin:$PATH Also, take a look at the slurmy config setup .","title":"Slurmy"},{"location":"#slurmy-special-handler-for-universal-running-of-multiple-jobs-yes","text":"Slurmy is a general batch submission module, which allows to define very general jobs to be run on batch system setups on linux computing clusters. Currently, only Slurm and HTCondor are supported as backends, but further backends can easily be added. The definition of the job execution is done with a general shell execution script, as is used by most batch systems. In addition to the batch definition, jobs can also be dynamically executed locally, which allows for an arbitrary combination of batch and local jobs.","title":"SLURMY - Special handLer for Universal Running of Multiple jobs, Yes!"},{"location":"#installation","text":"First off, it is very much recommended to use slurmy in python3. While it is compatible with python2, some features only work (properly) in python3. You can either install slurmy using pip or directly check out the git repository.","title":"Installation"},{"location":"#pip","text":"Most likely you won't have root privileges on your machine, so you have to make a user pip installation: pip install --user slurmy Depending on your setup pip points to the python2 or python3 pip executable. In order to ensure that you're installing slurmy for python3 you can be more explicit: pip3 install --user slurmy This will install the slurmy module in ~/.local/lib/pythonX.Y/site-packages/ ( X.Y indicates the version of your python installation) and the slurmy executables in ~/.local/bin/ . In order to ensure that python properly finds the module and you can run the executables, you should add the respective paths to PYTHONPATH and PATH : MYPYTHONVERSION=python$(python3 -c \"from sys import version_info; print('{}.{}'.format(version_info.major, version_info.minor))\") export PYTHONPATH=~/.local/lib/$MYPYTHONVERSION/site-packages:$PYTHONPATH export PATH=~/.local/bin:$PATH","title":"pip"},{"location":"#github","text":"Clone the latest stable tag or (if you're brave) master branch locally: git clone https://github.com/Thomas-Maier/slurmy.git LATESTTAG=$(git describe --abbrev=0 --tags) git co $LATESTTAG Make sure to add the respository folder to PYTHONPATH and the bin/ folder to PATH ': ## Assuming that we are in the repository folder now export PYTHONPATH=$PWD:$PYTHONPATH export PATH=$PWD/bin:$PATH Also, take a look at the slurmy config setup .","title":"github"},{"location":"howto/","text":"General Usage You can just write a piece of python code that imports the required slurmy classes (e.g. JobHandler and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified. Below you'll find several examples of how your job configuration script can look like. You should also have a look at the interactive slurmy section to get an idea what you can do. Make sure to also take a look at the documentation of JobHandler , in particular JobHandler.add_job() . What you want to do before doing anything else You can (and should) specify a slurmy config file, which defines your default configuration of relevant slurmy properties. In particular you can define which batch system backend you want to use and how it should be configured. This safes you from having to specify this every single time you want to use slurmy. You just need to create a file ~/.slurmy with this content (which you might want to modify): bookkeeping = ~/.slurmy_bookkeeping workdir = ./ backend = Slurm editor = emacs -nw ## Slurm backend options Slurm.partition = ls-schaile #Slurm.clusters = #Slurm.qos = #Slurm.exclude = #Slurm.mem = #Slurm.time = #Slurm.export = Snapshots By default, slurmy will do bookkeeping of past JobHandler sessions. The information is stored in a json file, which is defined in the slurmy config. Snapshot making can be deactivated by passing the respective argument to JobHandler . Snapshot making is very useful, in particular if you want to make use of interactive slurmy . Simple example from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs() With explicit backend definition If you don't create a slurmy config file specifying the batch system backend to use, or if you want to specify backends for each job, then you can do the following. from slurmy import JobHandler, Slurm ## Set up the backend slurm = Slurm(partition = 'ls-schaile') ## Set up the JobHandler jh = JobHandler(backend = slurm) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job ### The backend can be individually set for each job slurm_job = Slurm(partition = 'ls-schaile', mem = '6000mb') jh.add_job(backend = slurm_job, run_script = run_script) ## Run all jobs jh.run_jobs() Using an already existing script file (with optional arguments) You can also just use an existing script file. In this case you just specify the file path as run_script . You can also specify additional arguments to be passed to the run script. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Specify path to run script on disk run_script = '/path/to/run_script' ## Additional arguments to be passed to the run script run_args = 'hans horst' ## Add a job jh.add_job(run_script = run_script, run_args = run_args) ## Run all jobs jh.run_jobs() Chaining Jobs with Tags Jobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content of job 1 run_script_1 = \"\"\" sleep 10 echo \"hans\" \"\"\" ## Add job 1 with tag \"hans\" jh.add_job(run_script = run_script_1, tags = 'hans') ## Define the run script content of job 2 run_script_2 = \"\"\" echo \"horst\" \"\"\" ## Add job 2 with parent_tag \"hans\" jh.add_job(run_script = run_script_2, parent_tags = 'hans') ## Run all jobs jh.run_jobs() The tags and parent_tags arguments of jh.add_job() can also be a list of tags, in order to assign multiple tags to a job at once. Additional uses of tags Tags can also be used to just organise jobs. In interactive slurmy you can easily print out only jobs which have a specified tag via JobContainer.print() (i.e. jh.jobs.print(tags = 'hans') for the example above). Also, if you properly installed the tqdm module (see the recommended setup ), slurmy will keep track of the job progress for each tag separately in addition to the overall progress. Steering evaluation of jobs processing status By default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it finished and was successful or not. However, you can change how slurmy will evaluate whether the job is finished or was successful. Job listener First a word on what the default evaluation setup of slurmy is. In general, jobs are designed to do their status evaluations themselves, i.e. the JobHandler asks the job for it's status. This can get very performance heavy if this is connected to a request to the batch system accounting/bookkeeping. By default, slurmy runs a Listener to collect the job information from the batch system and set it's exitcode when it's finished. Keep in mind that setting a finished_func (see below) will deactivate the listener and possibly slow down the job submission cycle (if you do something processing intensive for the evaluation). Define output file for success evaluation You can define the output of each job explicitly to change the success evaluation to check for the output to be present. From a technical side, this sets up a Listener which checks if the defined output files exist and sets the associated jobs status to SUCCESS if it finds them. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler(output_max_attempts = 5) ## Define the run script content run_script = \"\"\" touch ~/hans.txt \"\"\" ## Add a job, specifying the output of the job. jh.add_job(run_script = run_script, output = '~/hans.txt') ## Run all jobs jh.run_jobs() The output_max_attempts argument of the JobHandler defines how many attempts are made to find the output file for a given job that is in FINISHED state. By default it is set to 5, in order to avoid delayed availability of the output file in the underlying file system. FINISHED and SUCCESS trigger in the run_script You can also set triggers in the run_script to indicate at which point in the job processing it should be considered as FINISHED and/or SUCCESS. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" @SLURMY.FINISHED echo \"horst\" @SLURMY.SUCCESS \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs() From a technical point, @SLURMY.FINISHED and @SLURMY.SUCCESS are substituted by temporary files, which are created at these points in the script. The FINISHED/SUCCESS evaluation is then checking if these files exist. The file name associated to @SLURMY.SUCCESS is also set as output , if it's not already specified as add_job argument. Custom evaluations for success, finished, and post-execution You can define a custom finished condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If finished_func is defined during add_job, the custom definition will be used instead of the default one during the finished evaluation. This example uses the predefined FinishedTrigger class, which checks if the specified file exists on disk in order to evaluate whether if the job finished or not. from slurmy import JobHandler, FinishedTrigger ## Set up finished evaluation class finished_file = '~/hans.txt' ft = FinishedTrigger(finished_file) ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" touch {} \"\"\".format(finished_file) ## Add a job jh.add_job(run_script = run_script, finished_func = ft) ## Run all jobs jh.run_jobs() This will actually do the same as the @SLURM.FINISHED example above. PLEASE NOTE: If you only specify finished_func with a simple(fast) evaluation, it's likely that your job will fail. This is simply because by default, slurmy will still ask the batch system accounting for the exitcode of the job and it's very likely that it's not updated in time. The same is true if you only put @SLURM.FINISHED and not @SLURM.SUCCESS in your run_script . Only specifying a custom success_func (or only @SLURM.SUCCESS ) is generally fine, though. In the same way as finished_func , you can also define success_func , to evaluate if a job is successful, or post_func , to define a post-processing which will be done locally after the job's success evaluation was done. Regarding class definitions Due to technically reasons connected to the snapshot feature , your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it. Execute job payload in Singularity with run script wrappers It's not unlikely that you will need to run your code not in the OS of the batch system worker nodes, but rather in a singularity environment. Slurmy provides a wrapper that will ensure that the run script is executed inside singularity on the worker node. You only need to provide the path to the singularity image that should be used. from slurmy import JobHandler, SingularityWrapper ## Set up the JobHandler sw = SingularityWrapper('/path/to/singularity/image.img') jh = JobHandler(wrapper = sw) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs()","title":"HowTo"},{"location":"howto/#general-usage","text":"You can just write a piece of python code that imports the required slurmy classes (e.g. JobHandler and the backend class of your choice), defines jobs and calls the job submission. Job execution definitions can either be provided by already written batch shell scripts, or by defining the content of the shell script directly in your python code. For both cases, arguments that should be passed upon the execution to the scripts can also be specified. Below you'll find several examples of how your job configuration script can look like. You should also have a look at the interactive slurmy section to get an idea what you can do. Make sure to also take a look at the documentation of JobHandler , in particular JobHandler.add_job() .","title":"General Usage"},{"location":"howto/#what-you-want-to-do-before-doing-anything-else","text":"You can (and should) specify a slurmy config file, which defines your default configuration of relevant slurmy properties. In particular you can define which batch system backend you want to use and how it should be configured. This safes you from having to specify this every single time you want to use slurmy. You just need to create a file ~/.slurmy with this content (which you might want to modify): bookkeeping = ~/.slurmy_bookkeeping workdir = ./ backend = Slurm editor = emacs -nw ## Slurm backend options Slurm.partition = ls-schaile #Slurm.clusters = #Slurm.qos = #Slurm.exclude = #Slurm.mem = #Slurm.time = #Slurm.export =","title":"What you want to do before doing anything else"},{"location":"howto/#snapshots","text":"By default, slurmy will do bookkeeping of past JobHandler sessions. The information is stored in a json file, which is defined in the slurmy config. Snapshot making can be deactivated by passing the respective argument to JobHandler . Snapshot making is very useful, in particular if you want to make use of interactive slurmy .","title":"Snapshots"},{"location":"howto/#simple-example","text":"from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs()","title":"Simple example"},{"location":"howto/#with-explicit-backend-definition","text":"If you don't create a slurmy config file specifying the batch system backend to use, or if you want to specify backends for each job, then you can do the following. from slurmy import JobHandler, Slurm ## Set up the backend slurm = Slurm(partition = 'ls-schaile') ## Set up the JobHandler jh = JobHandler(backend = slurm) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job ### The backend can be individually set for each job slurm_job = Slurm(partition = 'ls-schaile', mem = '6000mb') jh.add_job(backend = slurm_job, run_script = run_script) ## Run all jobs jh.run_jobs()","title":"With explicit backend definition"},{"location":"howto/#using-an-already-existing-script-file-with-optional-arguments","text":"You can also just use an existing script file. In this case you just specify the file path as run_script . You can also specify additional arguments to be passed to the run script. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Specify path to run script on disk run_script = '/path/to/run_script' ## Additional arguments to be passed to the run script run_args = 'hans horst' ## Add a job jh.add_job(run_script = run_script, run_args = run_args) ## Run all jobs jh.run_jobs()","title":"Using an already existing script file (with optional arguments)"},{"location":"howto/#chaining-jobs-with-tags","text":"Jobs can be connected by adding tags and parent tags to them. Jobs with parent tags X,Y, and Z will only be executed if all jobs that have the tags X, Y, or Z have successfully finished. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content of job 1 run_script_1 = \"\"\" sleep 10 echo \"hans\" \"\"\" ## Add job 1 with tag \"hans\" jh.add_job(run_script = run_script_1, tags = 'hans') ## Define the run script content of job 2 run_script_2 = \"\"\" echo \"horst\" \"\"\" ## Add job 2 with parent_tag \"hans\" jh.add_job(run_script = run_script_2, parent_tags = 'hans') ## Run all jobs jh.run_jobs() The tags and parent_tags arguments of jh.add_job() can also be a list of tags, in order to assign multiple tags to a job at once.","title":"Chaining Jobs with Tags"},{"location":"howto/#additional-uses-of-tags","text":"Tags can also be used to just organise jobs. In interactive slurmy you can easily print out only jobs which have a specified tag via JobContainer.print() (i.e. jh.jobs.print(tags = 'hans') for the example above). Also, if you properly installed the tqdm module (see the recommended setup ), slurmy will keep track of the job progress for each tag separately in addition to the overall progress.","title":"Additional uses of tags"},{"location":"howto/#steering-evaluation-of-jobs-processing-status","text":"By default, the exitcode of the job (either taken from the local process or from the batch system bookkeeping) is taken to determine if it finished and was successful or not. However, you can change how slurmy will evaluate whether the job is finished or was successful.","title":"Steering evaluation of jobs processing status"},{"location":"howto/#job-listener","text":"First a word on what the default evaluation setup of slurmy is. In general, jobs are designed to do their status evaluations themselves, i.e. the JobHandler asks the job for it's status. This can get very performance heavy if this is connected to a request to the batch system accounting/bookkeeping. By default, slurmy runs a Listener to collect the job information from the batch system and set it's exitcode when it's finished. Keep in mind that setting a finished_func (see below) will deactivate the listener and possibly slow down the job submission cycle (if you do something processing intensive for the evaluation).","title":"Job listener"},{"location":"howto/#define-output-file-for-success-evaluation","text":"You can define the output of each job explicitly to change the success evaluation to check for the output to be present. From a technical side, this sets up a Listener which checks if the defined output files exist and sets the associated jobs status to SUCCESS if it finds them. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler(output_max_attempts = 5) ## Define the run script content run_script = \"\"\" touch ~/hans.txt \"\"\" ## Add a job, specifying the output of the job. jh.add_job(run_script = run_script, output = '~/hans.txt') ## Run all jobs jh.run_jobs() The output_max_attempts argument of the JobHandler defines how many attempts are made to find the output file for a given job that is in FINISHED state. By default it is set to 5, in order to avoid delayed availability of the output file in the underlying file system.","title":"Define output file for success evaluation"},{"location":"howto/#finished-and-success-trigger-in-the-run_script","text":"You can also set triggers in the run_script to indicate at which point in the job processing it should be considered as FINISHED and/or SUCCESS. from slurmy import JobHandler ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" echo \"hans\" @SLURMY.FINISHED echo \"horst\" @SLURMY.SUCCESS \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs() From a technical point, @SLURMY.FINISHED and @SLURMY.SUCCESS are substituted by temporary files, which are created at these points in the script. The FINISHED/SUCCESS evaluation is then checking if these files exist. The file name associated to @SLURMY.SUCCESS is also set as output , if it's not already specified as add_job argument.","title":"FINISHED and SUCCESS trigger in the run_script"},{"location":"howto/#custom-evaluations-for-success-finished-and-post-execution","text":"You can define a custom finished condition by creating a dedicated class with __call__ defined. The function has to have exactly one argument, which is the config instance of the job. If finished_func is defined during add_job, the custom definition will be used instead of the default one during the finished evaluation. This example uses the predefined FinishedTrigger class, which checks if the specified file exists on disk in order to evaluate whether if the job finished or not. from slurmy import JobHandler, FinishedTrigger ## Set up finished evaluation class finished_file = '~/hans.txt' ft = FinishedTrigger(finished_file) ## Set up the JobHandler jh = JobHandler() ## Define the run script content run_script = \"\"\" touch {} \"\"\".format(finished_file) ## Add a job jh.add_job(run_script = run_script, finished_func = ft) ## Run all jobs jh.run_jobs() This will actually do the same as the @SLURM.FINISHED example above. PLEASE NOTE: If you only specify finished_func with a simple(fast) evaluation, it's likely that your job will fail. This is simply because by default, slurmy will still ask the batch system accounting for the exitcode of the job and it's very likely that it's not updated in time. The same is true if you only put @SLURM.FINISHED and not @SLURM.SUCCESS in your run_script . Only specifying a custom success_func (or only @SLURM.SUCCESS ) is generally fine, though. In the same way as finished_func , you can also define success_func , to evaluate if a job is successful, or post_func , to define a post-processing which will be done locally after the job's success evaluation was done.","title":"Custom evaluations for success, finished, and post-execution"},{"location":"howto/#regarding-class-definitions","text":"Due to technically reasons connected to the snapshot feature , your custom class definition must be known to python on your machine. The best way to ensure that is to make the definition known to python via PYTHONPATH. In principle you can just use a local function definition instead of a callable class if you don't want to use the snapshot feature. However, it is highly recommended to make use of it.","title":"Regarding class definitions"},{"location":"howto/#execute-job-payload-in-singularity-with-run-script-wrappers","text":"It's not unlikely that you will need to run your code not in the OS of the batch system worker nodes, but rather in a singularity environment. Slurmy provides a wrapper that will ensure that the run script is executed inside singularity on the worker node. You only need to provide the path to the singularity image that should be used. from slurmy import JobHandler, SingularityWrapper ## Set up the JobHandler sw = SingularityWrapper('/path/to/singularity/image.img') jh = JobHandler(wrapper = sw) ## Define the run script content run_script = \"\"\" echo \"hans\" \"\"\" ## Add a job jh.add_job(run_script = run_script) ## Run all jobs jh.run_jobs()","title":"Execute job payload in Singularity with run script wrappers"},{"location":"interactive_slurmy/preamble/","text":"You can use the slurmy executable to start an interactive slurmy session, which allows to interact with past JobHandler sessions or start new ones. Usage from slurmy --help : usage: slurmy [-h] [-p PATH] [-c CONFIG] [-t] [--debug] Slurmy interactive optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Path to the base folder of an existing JobHandler session. Directly loads the JobHandler as \"jh\". -c CONFIG, --config CONFIG Path to a job configuration file. -t Switch to start in test mode. --debug Run in debugging mode. If you prefer to use python2 (not recommended), you can also run the slurmy2 executable. If no argument is passed to the slurmy executable, it tries to load the latest session according to the bookkeeping and load it as jh . Example usage In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing JobHandler session. Just executing slurmy will bring up the latest JobHandler session: In [1]: jh Out[1]: MyAnalysis_1531405633 Every JobHandler has a member jobs which keeps track of all it's attached jobs: In [2]: jh.jobs Out[2]: Job \"ttbar\": CONFIGURED Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"data\": CONFIGURED ------------ CONFIGURED(4) As you can see, the JobHandler in this case has four jobs named \"data\", \"ttbar\", \"wjets\", and \"ww\", which is in the CONFIGURED state. Every job is attached as property to JobHandler .jobs, which provides a direct handle to access them: In [3]: jh.jobs.ww Out[3]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} Alternatively, jobs can be accessed directly by name via the JobHandler itself: In [4]: jh['ww'] Out[4]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} JobHandler .jobs also has a status_ property for every possible job status, which will print all jobs which currently are in this status: In [5]: jh.jobs.status_CONFIGURED Job \"wjets\": CONFIGURED Job \"data\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED In [6]: jh.jobs.status_RUNNING In [7]: As you've seen above, the job \"ww\" (and \"ttbar\" and \"wjets\" for that matter) has a tag \"bkg\", which was attached to the job via the tags option of JobHandler.add_job() . You can get the printout for jobs only tagged with \"bkg\" by calling the JobContainer.print() method: In [7]: jh.jobs.print(tags='bkg') Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED ------------ CONFIGURED(3) In this example, all jobs are in the CONFIGURED state so we can run the job submission with JobHandler.run_jobs() : In [8]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=3, F=1, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=2, F=1, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=0, F=1, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (3/0/3) failed (batch/local/all): (1/0/1) Time spent: 12.4 s You can see that this produces two different printouts. During the processing you'll get progress bars which indicate how many jobs are completed. On the very right of these progess bars you can also see how many jobs ended up in the SUCCESS(S), FAILED(F), or CANCELLED(C) state. You can also see that for each tag that is introduced with JobHandler.add_job() one progress bar is displayed, which keeps track of the jobs assigned with this tag. Slurmy will also evaluate the tag hierarchy dependent on how tags were assigned to jobs and order them accordingly in this printout. In this example, each job has it's own name as tag and \"ww\", \"ttbar\", and \"wjets\" have \"bkg\" as an additional tag. As you can see from the printout above, job \"ww\" ended up in FAILED state. In [9]: jh.jobs.ww Out[9]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: FAILED Tags: {'bkg', 'ww'} We can access the log file of the job directly with Job.log (which opens the log file with less ), in order to find out what went wrong: In [10]: jh.jobs.ww.log Usually, you probably want to fix your job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly: In [11]: jh.jobs.ww.edit_script() If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing retry = True to run_jobs : In [12]: jh.run_jobs(retry = True) all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 2.5 s The job \"ww\" is now in SUCCESS state (from fixing the run script before retrying the job): In [13]: jh.jobs.ww Out[13]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: SUCCESS Tags: {'bkg', 'ww'} Finally, if you want to start from a clean slate you can reset the JobHandler completely: In [14]: jh.reset() In [15]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 11.3 s In this case you actually might want to start again from the job configuration script that you wrote for your job submission. Have a look at the JobHandler and Job documentation to see what you can execute in interactive slurmy. Job configuration file The job definition file passed with -c is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the HowTo section, you can pass it to interactive slurmy. Interactive slurmy functions The interactive slurmy session also defines a couple of functions.","title":"Preamble"},{"location":"interactive_slurmy/preamble/#example-usage","text":"In general you can do everything in interactive slurmy that you can also do in python file which handles your job definition. On top of that you can easily inspect and manipulate an already existing JobHandler session. Just executing slurmy will bring up the latest JobHandler session: In [1]: jh Out[1]: MyAnalysis_1531405633 Every JobHandler has a member jobs which keeps track of all it's attached jobs: In [2]: jh.jobs Out[2]: Job \"ttbar\": CONFIGURED Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"data\": CONFIGURED ------------ CONFIGURED(4) As you can see, the JobHandler in this case has four jobs named \"data\", \"ttbar\", \"wjets\", and \"ww\", which is in the CONFIGURED state. Every job is attached as property to JobHandler .jobs, which provides a direct handle to access them: In [3]: jh.jobs.ww Out[3]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} Alternatively, jobs can be accessed directly by name via the JobHandler itself: In [4]: jh['ww'] Out[4]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: CONFIGURED Tags: {'bkg', 'ww'} JobHandler .jobs also has a status_ property for every possible job status, which will print all jobs which currently are in this status: In [5]: jh.jobs.status_CONFIGURED Job \"wjets\": CONFIGURED Job \"data\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED In [6]: jh.jobs.status_RUNNING In [7]: As you've seen above, the job \"ww\" (and \"ttbar\" and \"wjets\" for that matter) has a tag \"bkg\", which was attached to the job via the tags option of JobHandler.add_job() . You can get the printout for jobs only tagged with \"bkg\" by calling the JobContainer.print() method: In [7]: jh.jobs.print(tags='bkg') Job \"wjets\": CONFIGURED Job \"ww\": CONFIGURED Job \"ttbar\": CONFIGURED ------------ CONFIGURED(3) In this example, all jobs are in the CONFIGURED state so we can run the job submission with JobHandler.run_jobs() : In [8]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=3, F=1, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=2, F=1, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=0, F=1, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (3/0/3) failed (batch/local/all): (1/0/1) Time spent: 12.4 s You can see that this produces two different printouts. During the processing you'll get progress bars which indicate how many jobs are completed. On the very right of these progess bars you can also see how many jobs ended up in the SUCCESS(S), FAILED(F), or CANCELLED(C) state. You can also see that for each tag that is introduced with JobHandler.add_job() one progress bar is displayed, which keeps track of the jobs assigned with this tag. Slurmy will also evaluate the tag hierarchy dependent on how tags were assigned to jobs and order them accordingly in this printout. In this example, each job has it's own name as tag and \"ww\", \"ttbar\", and \"wjets\" have \"bkg\" as an additional tag. As you can see from the printout above, job \"ww\" ended up in FAILED state. In [9]: jh.jobs.ww Out[9]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: FAILED Tags: {'bkg', 'ww'} We can access the log file of the job directly with Job.log (which opens the log file with less ), in order to find out what went wrong: In [10]: jh.jobs.ww.log Usually, you probably want to fix your job configuration setup to fix a systematic problem in the job's run script creation. However, you can edit the run script directly: In [11]: jh.jobs.ww.edit_script() If any of the jobs ended up in FAILED or CANCELLED state, they can be retried by passing retry = True to run_jobs : In [12]: jh.run_jobs(retry = True) all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 2.5 s The job \"ww\" is now in SUCCESS state (from fixing the run script before retrying the job): In [13]: jh.jobs.ww Out[13]: Job \"ww\" Type: BATCH Backend: Slurm Script: /home/t/Thomas.Maier/testSlurmy/MyAnalysis_1531405633/scripts/ww Status: SUCCESS Tags: {'bkg', 'ww'} Finally, if you want to start from a clean slate you can reset the JobHandler completely: In [14]: jh.reset() In [15]: jh.run_jobs() all: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [, S=4, F=0, C=0] -data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] -bkg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [, S=3, F=0, C=0] --ttbar: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --wjets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] --ww: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [, S=1, F=0, C=0] Jobs processed (batch/local/all): (4/0/4) successful (batch/local/all): (4/0/4) Time spent: 11.3 s In this case you actually might want to start again from the job configuration script that you wrote for your job submission. Have a look at the JobHandler and Job documentation to see what you can execute in interactive slurmy.","title":"Example usage"},{"location":"interactive_slurmy/preamble/#job-configuration-file","text":"The job definition file passed with -c is a convenient way to make job definitions. Inside the slurmy session, all necessary imports, like JobHandler and the backend classes, are already provided. This allows for skimmed down JobHandler setups that then can be further interacted with (you can omit import statements). As long as your definition file is \"flat\" (no encapsulated definitions), i.e. like the examples given in the HowTo section, you can pass it to interactive slurmy.","title":"Job configuration file"},{"location":"interactive_slurmy/preamble/#interactive-slurmy-functions","text":"The interactive slurmy session also defines a couple of functions.","title":"Interactive slurmy functions"}]}